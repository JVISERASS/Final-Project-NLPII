{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5a044c",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ML Libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path('.').absolute().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e9da23",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load project configuration\n",
    "config_path = PROJECT_ROOT / 'configs' / 'config.yaml'\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üìã Project Configuration:\")\n",
    "print(f\"  Model: {config['model']['base_model']}\")\n",
    "print(f\"  Dataset: {config['data']['dataset_name']}\")\n",
    "print(f\"  LoRA Rank: {config['lora']['rank']}\")\n",
    "print(f\"  Batch Size: {config['training']['batch_size']}\")\n",
    "print(f\"  Learning Rate: {config['training']['learning_rate']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7783d9",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Databricks Dolly 15k dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"  Total samples: {len(dataset['train'])}\")\n",
    "print(f\"  Features: {dataset['train'].features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f87a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Calculate lengths\n",
    "df['instruction_len'] = df['instruction'].str.len()\n",
    "df['context_len'] = df['context'].str.len()\n",
    "df['response_len'] = df['response'].str.len()\n",
    "df['has_context'] = df['context'].str.len() > 0\n",
    "\n",
    "print(\"\\nüìà Length Statistics:\")\n",
    "print(df[['instruction_len', 'context_len', 'response_len']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Category counts\n",
    "category_counts = df['category'].value_counts()\n",
    "axes[0].barh(category_counts.index, category_counts.values, color='steelblue')\n",
    "axes[0].set_xlabel('Count')\n",
    "axes[0].set_title('Samples per Category', fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Context presence\n",
    "context_counts = df['has_context'].value_counts()\n",
    "axes[1].pie(context_counts.values, labels=['With Context', 'No Context'], \n",
    "            autopct='%1.1f%%', colors=['#4ECDC4', '#FF6B6B'])\n",
    "axes[1].set_title('Samples with Context', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9686a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, col, title in zip(axes, \n",
    "                         ['instruction_len', 'context_len', 'response_len'],\n",
    "                         ['Instruction Length', 'Context Length', 'Response Length']):\n",
    "    data = df[col]\n",
    "    ax.hist(data, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.0f}')\n",
    "    ax.axvline(data.median(), color='green', linestyle='--', label=f'Median: {data.median():.0f}')\n",
    "    ax.set_xlabel('Characters')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample examples by category\n",
    "print(\"\\nüìù Sample Examples:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category in df['category'].unique()[:3]:\n",
    "    sample = df[df['category'] == category].iloc[0]\n",
    "    print(f\"\\nüè∑Ô∏è Category: {category}\")\n",
    "    print(f\"üìå Instruction: {sample['instruction'][:200]}...\")\n",
    "    print(f\"üí¨ Response: {sample['response'][:200]}...\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6105ffad",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb00e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data preparation module\n",
    "from data.prepare_data import DatasetPreparator\n",
    "\n",
    "# Prepare data\n",
    "preparator = DatasetPreparator(config)\n",
    "datasets = preparator.prepare_data()\n",
    "\n",
    "print(\"\\n‚úÖ Prepared datasets:\")\n",
    "for split, ds in datasets.items():\n",
    "    print(f\"  {split}: {len(ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show formatted prompt example\n",
    "sample = datasets['train'][0]\n",
    "print(\"üìù Formatted Prompt Example:\")\n",
    "print(\"=\"*80)\n",
    "print(sample['text'][:1000])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2513f5",
   "metadata": {},
   "source": [
    "## 5. Model Loading & Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff25f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "model_name = config['model']['base_model']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"\\nüî§ Tokenizer Info:\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Model max length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9619c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token lengths\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, truncation=False))\n",
    "\n",
    "# Sample 1000 examples for speed\n",
    "sample_df = df.sample(min(1000, len(df)), random_state=42)\n",
    "\n",
    "sample_df['total_tokens'] = sample_df.apply(\n",
    "    lambda x: count_tokens(x['instruction'] + ' ' + x['context'] + ' ' + x['response']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Token Statistics:\")\n",
    "print(sample_df['total_tokens'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c41b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token distribution plot\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.hist(sample_df['total_tokens'], bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(2048, color='red', linestyle='--', linewidth=2, label='Max length (2048)')\n",
    "ax.axvline(sample_df['total_tokens'].mean(), color='green', linestyle='--', \n",
    "           label=f'Mean: {sample_df[\"total_tokens\"].mean():.0f}')\n",
    "\n",
    "ax.set_xlabel('Total Tokens')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Token Length Distribution', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Truncation analysis\n",
    "truncated = (sample_df['total_tokens'] > 2048).sum()\n",
    "print(f\"\\n‚ö†Ô∏è Samples exceeding 2048 tokens: {truncated} ({truncated/len(sample_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4715f",
   "metadata": {},
   "source": [
    "## 6. Quick Inference Test (Base Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9892e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model for comparison\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"Loading base model with 4-bit quantization...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model loaded!\")\n",
    "print(f\"  Parameters: {base_model.num_parameters():,}\")\n",
    "print(f\"  Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d4249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "def generate_response(prompt, model, tokenizer, max_new_tokens=256):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"<|im_start|>user\\nExplain what machine learning is in simple terms.<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "print(\"ü§ñ Base Model Response:\")\n",
    "print(\"=\"*80)\n",
    "response = generate_response(test_prompt, base_model, tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb16394",
   "metadata": {},
   "source": [
    "## 7. Results Loading & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results (after running main.py)\n",
    "results_dir = PROJECT_ROOT / 'results'\n",
    "\n",
    "# Find latest results file\n",
    "results_files = list(results_dir.glob('pipeline_results_*.json'))\n",
    "\n",
    "if results_files:\n",
    "    latest_results = max(results_files, key=os.path.getctime)\n",
    "    print(f\"üìÅ Loading results from: {latest_results}\")\n",
    "    \n",
    "    with open(latest_results, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"\\nüìä Available results:\")\n",
    "    for key in results.keys():\n",
    "        print(f\"  - {key}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results found. Run main.py first.\")\n",
    "    results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualizer\n",
    "from visualize import ResultsVisualizer\n",
    "\n",
    "if results:\n",
    "    viz = ResultsVisualizer(output_dir=str(results_dir / 'visualizations'))\n",
    "    \n",
    "    # Generate benchmark visualizations if available\n",
    "    if 'benchmark' in results and results['benchmark']:\n",
    "        print(\"\\nüìä Benchmark Results:\")\n",
    "        fig = viz.plot_benchmark_combined(results['benchmark'])\n",
    "        plt.show()\n",
    "    \n",
    "    # Generate quality metrics if available\n",
    "    if 'evaluation' in results and results['evaluation']:\n",
    "        print(\"\\nüìà Quality Metrics:\")\n",
    "        fig = viz.plot_quality_metrics(results['evaluation'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9e07e",
   "metadata": {},
   "source": [
    "## 8. Custom Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1570a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your custom analysis here\n",
    "# This cell is for exploratory work and custom experiments\n",
    "\n",
    "print(\"üî¨ Custom Analysis Section\")\n",
    "print(\"Add your own analysis code here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce9e4d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes & Observations\n",
    "\n",
    "Use this section to document your findings and observations during the analysis.\n",
    "\n",
    "### Key Findings:\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "\n",
    "### Recommendations:\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
